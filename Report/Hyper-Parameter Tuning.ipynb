{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3795cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from pathlib import Path\n",
    "import os\n",
    "import fintex_functions\n",
    "import importlib\n",
    "importlib.reload(fintex_functions)\n",
    "from fintex_functions import analogy_function, most_similar_function, unmatched_function, word_embedding_2d_representation\n",
    "from fintex_functions import single_word_embedding_2d_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cadf07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjo\\AppData\\Local\\Temp\\ipykernel_14796\\4216099227.py:5: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\benjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot, Tokenizer \n",
    "from keras.utils import pad_sequences \n",
    "from keras.models import Sequential, Model\n",
    "# from keras.layers.core import\n",
    "from keras.layers import Flatten, Input, Concatenate, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, GRU, MultiHeadAttention, Add, Bidirectional,  Activation, Dropout, Dense, GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "import json\n",
    "from keras.optimizers import Adam\n",
    "# from tensorflow.keras.utils import serialize_keras_object\n",
    "from tensorflow.keras.utils import serialize_keras_object\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import clone_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129c0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214a118",
   "metadata": {},
   "source": [
    "## Load in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc8f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_news_1 = pd.read_csv('fin_news_optimized_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460df828",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='labels', data=fin_news_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f5a85",
   "metadata": {},
   "source": [
    "## Load text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_folder_path = \"C:\\\\Users\\\\benjo\\\\Final Project\\\\Assets\\\\Word embedings\\\\\"\n",
    "file_location = 'FinText_Word2Vec_CBOW\\\\Word_Embedding_2000_2015'\n",
    "\n",
    "FinText_Word2Vec_cbow = Word2Vec.load(embedding_folder_path + file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d5afa",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c26050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text_):\n",
    "\n",
    "    # Remove links, mentions     \n",
    "    # reduce all text to lower case\n",
    "    text_ = text_.lower()\n",
    "\n",
    "    #Remove mentions\n",
    "    text_ = re.sub('@\\S+', ' ', text_)\n",
    "    \n",
    "    #Remove any links\n",
    "    text_ = re.sub('https?:\\S+|http?:\\S', ' ', text_)\n",
    "    \n",
    "    # Remove puntuations and numbers\n",
    "    text_ = re.sub('[^a-zA-Z]', ' ', text_)\n",
    "\n",
    "    # Single character removal \n",
    "    text_ = re.sub(r\"\\s+[a-zA-z]\\s+\", ' ', text_)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text_ = re.sub(r'\\s+', ' ', text_)\n",
    "    \n",
    "    # Lematize\n",
    "#     tokens_result = tokenizer.tokenize(tweet_)\n",
    "#     pos_tuple_list = nltk.pos_tag(tokens_result)\n",
    "#     POS_TRANSLATED = \n",
    "#     POS_TRANS()\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens_result]\n",
    "\n",
    "#     combined_sentence = \" \".join(lemmatized_tokens)\n",
    "#     pattern_ = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "#     tweet_ = pattern_.sub('', tweet_)\n",
    "\n",
    "    return text_\n",
    "\n",
    "\n",
    "def Xtrain_pre_processing(x_feature):\n",
    "    feature = []\n",
    "    text_ = list(x_feature)\n",
    "    for txt in text_:\n",
    "        sent = preprocess_sentence(txt)\n",
    "#         sent = clean_text(txt)\n",
    "        \n",
    "#         extr_con_val = extract_concept_value_pairs(sent)\n",
    "        feature.append(sent)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf26ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xtrain_pre_processing(fin_news_optimized_6['sentence'])\n",
    "fin_news_optimized_6['labels'] = fin_news_optimized_6['labels'].astype(int)\n",
    "y = fin_news_optimized_6['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_train, X0_test, y0_train, y0_test =  train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4f8b6",
   "metadata": {},
   "source": [
    "## Preparing the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2aa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "word_tokenizer.fit_on_texts(X0_train)\n",
    "\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabccef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This function prepares the embedding layer for our NN, it takes in the feature input and returns a tokenized array with padding \n",
    "def prep_embedding_layer(feature_, posit_of_padd_, max_padd_len):\n",
    "\n",
    "\n",
    "    feature_ = word_tokenizer.texts_to_sequences(feature_)\n",
    "\n",
    "    feature_ = pad_sequences(feature_, padding=posit_of_padd_, maxlen=max_padd_len, truncating='post')\n",
    "    \n",
    "    return feature_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc949af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_ = FinText_Word2Vec_cbow.wv['word'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_train_emb = prep_embedding_layer(X0_train, 'post', max_length_)\n",
    "X0_test_emb = prep_embedding_layer(X0_test, 'post', max_length_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee72450",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_train_OHE =  tensorflow.keras.utils.to_categorical(y0_train, num_classes=3)\n",
    "y0_test_OHE =  tensorflow.keras.utils.to_categorical(y0_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e760c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X0_train_emb.shape, X0_test_emb.shape ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfa0f8",
   "metadata": {},
   "source": [
    "## Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3cbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "FinText_Word2Vec_cbow.wv['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FinText_Word2Vec_cbow.wv['amazon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa2b46",
   "metadata": {},
   "source": [
    "Length should be == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b644642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_FinText(vocabSize, maxLength, wordInd, word_emb_model ):\n",
    "    matrix = np.zeros((vocabSize, maxLength))  \n",
    "    for word, i in wordInd.items():\n",
    "        if word in FinText_Word2Vec_cbow.wv: \n",
    "            vector_ = word_emb_model[word]\n",
    "            if vector_ is not None:\n",
    "                matrix[i] = vector_\n",
    "            \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = initialize_FinText(vocab_size, max_length_, word_tokenizer.word_index, FinText_Word2Vec_cbow.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd98fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_train_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_matrix.shape[1] == 300:\n",
    "    pritn('True')\n",
    "    embedding_dim = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f9019",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5cbf",
   "metadata": {},
   "source": [
    "### Define Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb2af6",
   "metadata": {},
   "source": [
    "### Derive balancing weight values\n",
    "Due to the datasets unbalanced nature, we would need to add class weights in order to eliminate bias towards the class that holds the highest volumen of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y0_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7def003",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_train_emb = np.array(y0_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y0_train_emb), y=y0_train_emb)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ff4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8bc52a",
   "metadata": {},
   "source": [
    "### Creating the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(MyAttention, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)  # Use self.V as a layer\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MyAttention, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f60482",
   "metadata": {},
   "source": [
    "### Create model (Viklund) test \n",
    "\n",
    "We just want to see if the model architecture is the same as the one that is designed in my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c66c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Embedding Layer\n",
    "sequence_input = Input(shape=(X0_train_emb.shape[1],), dtype=\"int32\")\n",
    "embedded_sequences = Embedding(vocab_size, \n",
    "                               embedding_dim,\n",
    "                               weights=[embedding_matrix],\n",
    "                               trainable = False)(sequence_input)\n",
    "\n",
    "# Add Bidirectional layer\n",
    "bilstm = Bidirectional(LSTM(64, return_sequences=True), name=\"bi_lstm_0\")(embedded_sequences)\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(32, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(bilstm)\n",
    "\n",
    "\n",
    "# Concatenate RNN hidden states and apply attention\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = MyAttention(10)(lstm, state_h)\n",
    "\n",
    "# Add Dense Layers\n",
    "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
    "dropout = Dropout(0.05)(dense1)\n",
    "output = Dense(3, activation=\"softmax\")(dropout)\n",
    "\n",
    "\n",
    "# Create the Model object\n",
    "Bi_ATT_2_model = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# Plot Model\n",
    "# keras.utils.plot_model(Bi_AT_model, show_shapes=True, dpi=90)\n",
    "Bi_ATT_2_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa9f67",
   "metadata": {},
   "source": [
    "## Build Hyper Parameter Tuning Helper Fucntion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef723fe",
   "metadata": {},
   "source": [
    "Split dataset AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train_emb, X1_val_emb, y1_train_OHE, y1_val_OHE =  train_test_split(X0_train_emb, y0_train_OHE, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6688a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_bi_att_2_model(vocab_size, embedding_matrix, params):\n",
    "    # Add Embedding Layer\n",
    "    sequence_input = Input(shape=(X0_train_emb.shape[1],), dtype=\"int32\")\n",
    "    embedded_sequences = Embedding(vocab_size, \n",
    "                                   embedding_matrix.shape[1],  # Use the embedding_matrix dimension\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   trainable=False)(sequence_input)\n",
    "\n",
    "    # Add Bidirectional layer\n",
    "    lstm_layers = []\n",
    "    for _ in range(params['num_lstm_layers']):\n",
    "        bilstm = Bidirectional(LSTM(params['lstm_units'], return_sequences=True), name=f\"bi_lstm_{_}\")(embedded_sequences)\n",
    "        lstm_layers.append(bilstm)\n",
    "        embedded_sequences = bilstm  # Pass output as input to the next layer\n",
    "\n",
    "    (lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(params['lstm_units'], return_sequences=True, return_state=True), name=\"bi_lstm_final\")(embedded_sequences)\n",
    "\n",
    "    # Concatenate RNN hidden states and apply attention\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    context_vector, attention_weights = MyAttention(params['attention_units'])(lstm, state_h)\n",
    "\n",
    "    # Add Dense Layers\n",
    "    dense1 = Dense(params['units'], activation=\"relu\")(context_vector)\n",
    "    dropout = Dropout(params['dropout_rate'])(dense1)\n",
    "    output = Dense(3, activation=\"softmax\")(dropout)\n",
    "\n",
    "    # Create the Model object\n",
    "    model = Model(inputs=sequence_input, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_random_search_model(hp):\n",
    "    params = {\n",
    "        'num_lstm_layers': hp.Choice('num_lstm_layers', values=[1, 2, 3]),  # Number of LSTM layers\n",
    "        'learning_rate': hp.Choice('learning_rate', values=[0.001, 0.01, 0.1]),\n",
    "        'batch_size': hp.Choice('batch_size', values=[32, 64, 128]),\n",
    "        'units': hp.Choice('units', values=[16, 32, 64]),\n",
    "        'dropout_rate': hp.Float('dropout_rate', min_value=0.2, max_value=0.4, step=0.1),\n",
    "        'lstm_units': hp.Choice('lstm_units', values=[32, 64, 128]),\n",
    "        'attention_units': hp.Choice('attention_units', values=[8, 16, 32])\n",
    "    }\n",
    "    \n",
    "    model = create_bi_att_2_model(vocab_size, embedding_matrix, params)\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', keras.metrics.AUC(name='AUC'), f1_m,precision_m, recall_m])\n",
    "    return model\n",
    "\n",
    "def random_search(x_tr, y_tr, x_val, y_val, num_trials_, obj_, cw_,early_stopping_,checkpointer_):\n",
    "    tuner = RandomSearch(\n",
    "        build_random_search_model,\n",
    "        objective=obj_,\n",
    "        max_trials=num_trials_,## Loading Embeddings\n",
    "        directory='random_search_logs',\n",
    "        project_name = 'Bi_ATT_random_search'\n",
    "    )\n",
    "\n",
    "    tuner.search(x=x_tr, \n",
    "                 y=y_tr, \n",
    "                 epochs=15, \n",
    "                 validation_data=(x_val , y_val ),\n",
    "                 callbacks=[early_stopping_, checkpointer_]\n",
    "                 class_weight=cw_ )\n",
    "\n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "    return best_model\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'num_lstm_layers': [1, 2, 3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'units': [16, 32, 64],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'lstm_units': [32, 64, 128],\n",
    "    'attention_units': [8, 16, 32]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8dec9",
   "metadata": {},
   "source": [
    "#### Save model in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_nn_path = \"C:\\\\Users\\\\benjo\\\\Final Project\\\\Saved_Bi_ATT_2_model\"\n",
    "\n",
    "monitor_acc = 'val_acc'\n",
    "monitor_auc = 'val_AUC'\n",
    "checkpointer = ModelCheckpoint(filepath=lstm_nn_path,\n",
    "                               verbose=1,\n",
    "                               monitor=monitor_auc,\n",
    "                               mode='max',\n",
    "                               save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=monitor_auc,\n",
    "                               patience=5,\n",
    "                               mode='max',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d40aab",
   "metadata": {},
   "source": [
    "### Begin Hyper-Param Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search(X1_train_emb, y1_train_OHE, X1_val_emb, y1_val_OHE, num_trials_=10, obj_='val_AUC', cw_=class_weight_dict, early_stopping, checkpointer)\n",
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
